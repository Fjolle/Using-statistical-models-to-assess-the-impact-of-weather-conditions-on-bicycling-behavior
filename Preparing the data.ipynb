{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries \n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import requests\n",
    "from xml.etree import ElementTree as ET\n",
    "import datetime\n",
    "format = \"%d/%m/%Y %H:%M\"\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import urllib\n",
    "from darksky import forecast\n",
    "from datetime import datetime\n",
    "import forecastio\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and clean the historical bike journey data from January 2019 to December 2019\n",
    "\n",
    "# Read list of names of all files from a separate CSV\n",
    "with open('tfl_bike_trips.csv', 'r') as f:\n",
    "    csv_list = f.read().splitlines()\n",
    "\n",
    "# Download all of the bike journey CSV files for 2019 and appending to one dataset\n",
    "website = 'http://cycling.data.tfl.gov.uk/usage-stats/'\n",
    "\n",
    "url_list = [website + urllib.parse.quote(x) for x in csv_list]\n",
    "dfs = (pd.read_csv(url) for url in url_list)\n",
    "bike_journey_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "#Format the date and build a unique ID using the date for later use in merging datasets\n",
    "bike_journey_data['Start Date Converted']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.date\n",
    "bike_journey_data['End Date Converted']= pd.to_datetime(bike_journey_data['End Date'], format=format).dt.date\n",
    "bike_journey_data['Hours']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.hour\n",
    "bike_journey_data['Week Day']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.weekday\n",
    "bike_journey_data['Day']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.day\n",
    "bike_journey_data['Month']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.month\n",
    "bike_journey_data['Year']= pd.to_datetime(bike_journey_data['Start Date'], format=format).dt.year\n",
    "bike_journey_data['Duration in minutes']=bike_journey_data['Duration']/60\n",
    "bike_journey_data['id'] = bike_journey_data['Year'].map(str) + '-' + bike_journey_data['Month'].map(str) + '-' + bike_journey_data['Day'].map(str)\n",
    "bike_journey_data[\"id\"] = bike_journey_data[\"id\"].astype(str)\n",
    "bike_journey_data['id_hours'] = bike_journey_data['Start Date Converted'].map(str)+ '-' + bike_journey_data['Hours'].map(str)\n",
    "bike_journey_data[\"id_hours\"] = bike_journey_data[\"id_hours\"].astype(str)\n",
    "bike_journey_data['new_date']=bike_journey_data['id_hours'].apply(lambda x: datetime.strptime(x, '%Y-%M-%d-%H').strftime('%Y-%B-%d-%H'))\n",
    "bike_journey_data.to_csv('bike_journey_data.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real time bike location data from TfL API\n",
    "\n",
    "locu_api='your api'\n",
    "\n",
    "url='https://api.tfl.gov.uk/BikePoint?app_id=app_id&app_key=your api key'\n",
    "json_obj=urllib.request.urlopen(url)\n",
    "\n",
    "data=json.load(json_obj)\n",
    "\n",
    "#Storing API data into lists and appending it to a dataframe\n",
    "value1 =[]\n",
    "for item1 in range(len(data)):\n",
    "    for item2 in range(len(data[item1]['additionalProperties'])):\n",
    "        if (data[item1]['additionalProperties'][item2]['key'])==\"NbDocks\":\n",
    "            value1.append(data[item1]['additionalProperties'][item2]['value'])\n",
    "\n",
    "value2 =[]\n",
    "for item1 in range(len(data)):\n",
    "    for item2 in range(len(data[item1]['additionalProperties'])):\n",
    "        if (data[item1]['additionalProperties'][item2]['key'])==\"NbEmptyDocks\":\n",
    "            value2.append(data[item1]['additionalProperties'][item2]['value'])\n",
    "            \n",
    "value3 =[]\n",
    "for item1 in range(len(data)):\n",
    "     value3.append(data[item1]['commonName'])\n",
    "        \n",
    "value4 =[]\n",
    "for item1 in range(len(data)):\n",
    "     value4.append(data[item1]['lat'])\n",
    "        \n",
    "value5 =[]\n",
    "for item1 in range(len(data)):\n",
    "     value5.append(data[item1]['lon'])\n",
    "        \n",
    "\n",
    "value6 =[]\n",
    "for item1 in range(len(data)):\n",
    "    for item2 in range(len(data[item1]['additionalProperties'])):\n",
    "        if (data[item1]['additionalProperties'][item2]['key'])==\"NbBikes\":\n",
    "            value6.append(data[item1]['additionalProperties'][item2]['value']) \n",
    "            \n",
    "value7 =[]\n",
    "for item1 in range(len(data)):\n",
    "     value7.append(data[item1]['id'])\n",
    "\n",
    "ID=[]\n",
    "for value in value7:\n",
    "    ID.append(value[11:])\n",
    "    \n",
    "         \n",
    "bike_location_data= pd.DataFrame(list(zip(value1, value2, value3, \n",
    "                                value4, value5, value6, ID)), \n",
    "                columns = [\"Capacity\",\"Empty docks\",\"Bike station\",\"Lat\", \"Lon\", \"Avail bikes\", \"id\"])\n",
    "\n",
    "bike_location_data[\"id\"] = bike_location_data[\"id\"].astype(int)\n",
    "\n",
    "bike_location_data.to_csv('bike_location_data', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'datetime.datetime' has no attribute 'datetime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-15d67140f578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m51.51\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.13\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2019\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mforecast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecastio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_forecast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"precipIntensity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"humidity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"windSpeed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"visibility\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'datetime.datetime' has no attribute 'datetime'"
     ]
    }
   ],
   "source": [
    "# Getting the weather data from the DarkSky API\n",
    "api_key = ('your api key')\n",
    "lat = 51.51\n",
    "lng = -0.13\n",
    "date = datetime.datetime(2019,1,1)\n",
    "forecast = forecastio.load_forecast(api_key, lat, lng, time=date)\n",
    "attributes = [\"temperature\", \"precipIntensity\", \"humidity\", \"windSpeed\", \"visibility\"]\n",
    "\n",
    "times = []\n",
    "data = {}\n",
    "for attr in attributes:\n",
    "    data[attr] = []\n",
    "\n",
    "start = datetime.datetime(2019, 1, 1)\n",
    "for offset in range(1, 365):\n",
    "    forecast = forecastio.load_forecast('your api key', '51.51', '-0.13', time=start+datetime.timedelta(offset), units=\"uk\")\n",
    "    h = forecast.hourly()\n",
    "    d = h.data\n",
    "    for p in d:\n",
    "        times.append(p.time)\n",
    "        try:\n",
    "            for i in attributes:\n",
    "                data[i].append(p.d[i])\n",
    "        except:\n",
    "            print(KeyError)\n",
    "\n",
    "weather_data = pd.DataFrame(data, index=times)\n",
    "\n",
    "weather_data.index.names = ['Date']\n",
    "\n",
    "#Format the date and build a unique ID using the date for later use in merging datasets\n",
    "weather_data['Date']=weather_data.index\n",
    "weather_data['Date Converted']= pd.to_datetime(weather_data['Date'], format=format).dt.date\n",
    "weather_data['Hours']= pd.to_datetime(weather_data['Date'], format=format).dt.hour\n",
    "weather_data[\"Hours\"].replace(0, 24)\n",
    "weather_data['Day']= pd.to_datetime(weather_data['Date'], format=format).dt.day\n",
    "weather_data['Week Day']= pd.to_datetime(weather_data['Date'], format=format).dt.weekday\n",
    "weather_data['Month']= pd.to_datetime(weather_data['Date'], format=format).dt.month\n",
    "weather_data['Year']= pd.to_datetime(weather_data['Date'], format=format).dt.year\n",
    "weather_data['id'] = weather_data['Year'].map(str) + '-' + weather_data['Month'].map(str) + '-' + weather_data['Day'].map(str).map(str)\n",
    "weather_data[\"id\"] = weather_data[\"id\"].astype(str)\n",
    "weather_data[\"Hours\"] = weather_data[\"Hours\"].astype(str)\n",
    "#weather_data=weather_data.drop(columns=['Day', 'Year', 'Date'])\n",
    "weather_data['id_Hours'] = weather_data['Date Converted'].map(str) + '-' + weather_data['Hours'].map(str)\n",
    "weather_data[\"id_Hours\"] = weather_data[\"id_Hours\"].astype(str)\n",
    "\n",
    "#%matplotlib inline\n",
    "#plt.style.use('ggplot')\n",
    "#weather_data.plot(subplots=True);\n",
    "\n",
    "weather_data.to_csv('weather_data_27_mar_2020.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine journey data with location data\n",
    "bike_data= bike_journey_data.merge(right =bike_location_data,\n",
    "                                   how = 'inner',\n",
    "                                   left_on = 'StartStation Id',\n",
    "                                   right_on = 'id')\n",
    "bike_data=bike_data.drop(columns=['id_y','Year'])\n",
    "bike_data.sort_values(by=['id_x'])'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
